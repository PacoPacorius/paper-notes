▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊
▊▊ Parallel Tag Cache for Multi-core Processors ▊▊
▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊

*** I. Introduction ***

* Tag cache between LLC and memory significantly reduces tag read/write 
    overhead in single-processor systems. Crucial for hardware-managed 
    tagged memory. A parallel TC supporting multi-core processors has 
    not yet been designed however

* We don't want sequential tag access. HTTs corrupt the filesystem in 
    multicore tag cache systems. We still want to use a hierarchical
    tag table for efficiency.


*** II. Related Work *** (aka existing tag cache architectures)

* Split tag caching: store tags in an array separate from data,
    allows for better compression (since most data is untagged or tagged zero
    anyway), but tougher to maintain data consistency. Data consistency issue
    is accentuated in multicore systems!

* Merged tag caching: store tags along with data. Most commonly used.
    Double the memory traffic!

* Solution: use tag cache between LLC and memory! But tag caches become too 
    big! 

* Solution 2: hardware assisted data flow isolation (HDFI). 8MB tag table and
    128kB meta tag table. 1 bit from the mtt associates with 64B of the tt, 
    if it is 0, the 64B are empty, don't store them. This saves a considerable
    amount of space! Save both tables in an updated CHERI uniform 2kB cache,
    still sequential cache though! Boooo!!


*** III. Storing and Caching Tags Using HTTs *** (HTT architecture overview)

Hierarchical Tag Tables are a compression scheme.

A. Layout in Memory: Hierarchical Tag Table

* Example:
1GB total memory
64-bit word corresponds to 2-bit tag
3-level HTT

Inside main memory, create 1024 * (2/64) = 32MB tag partition. 31MB are enough
to store all the tags. Tags are stored in a tag table with 2-byte width entries.
Every entry corresponds to 64B of data. Linear mapping between data and its 
tags. 

Remaining 1MB? Each bit of tag map 0 correspond to a tag table node that are 
g-bits wide (here, 512 bits). TM0 is 32*1024 / 512 = 64kB wide. 

Tag map 1 does the same to tag map 0 in turn, this needs only 
62*1024 / 512 = 124B.

* Whenever a TM0 bit is 0, the corresponding 64-bit tag word is either 0 or
    uninitialized, same goes for TM1 and TM0.

* This allows us to not store redundant 0s in the tag cache (since most tags
    will either be 0 or unused anyway), thus we achieve a compression scheme
    with 0 memory overhead.

B. Layout In Cache: Uniformed Tag Cache

* Do we use uniform or separate tag cache?

* Separate achieves best memory utilization when we know a-priori the 
    temporal and spatial locality of our applications but is bad for 
    generic processors.

* Uniform cache allows us to dynamically resize the occupied cache space.
    We are forced to use the same block size for all TT, TM0 and TM1.
    Our optimal g is also equal to our cache block size.

C. Overall Structure Of Proposed TC

* A LLC memory access requires a whole transaction of tag cache accesses
    (at least, if we access the HTT top-down, from TM1 to TT). A parallel
    cache needs every transaction to happen atomically.


*** IV. Further Improvement Of Efficiency ***

A. Avoid Redundant Store
* When a dirty cache block is written in memory, the attached tags may be 
    clean. Most LLC accesses are like this. Enforce read-before-write for
    all tag write accesses, improving performance and minimizing concurrency
    issues (blocking on conflicting tag accesses mostly).
     

B. Avoid Empty Access
* The TC can invalidate cache blocks when they become empty and create 
    empty cache blocks without fetching them from memory (???)
    Ideally we only store top-level blocks and non-top-level non-empty blocks
    in cache.


C. Search Order
* If we have mostly empty tag blocks, the top-down search order is best.
    However, if most of our tag blocks are non-empty, it would be better 
    to search bottom-up, starting from the TT.


D. Dynamically Select Search Order At Runtime
* We can dynamically change our search order depending on which level 
    (TT, TM0 or TM1) tag cache accesses are finally served. Optimal order
    for previous period is likely optimal for current period as well.


*** V. Maintaining data consistency ***

A. Data Consistency Issues (assuming naive parallel serving of tag cache accesses)
* Two main concurrency issues:
    a. a stale block is fetched from memory due to a late removal of a tag 
    map bit.
    b. double creation of a block due to an early initiation of a tag map bit.


B. Two-Phase Locking
* a. potentially updated blocks locked top-down - lock phase
    b. said blocks are written to and unlocked bottom-up  - unlock phase
    This scheme guarantees no cyclic dependencies, once a tag cache access
    unlocks one block, it cannot lock any more!

* Guarantees consistency, but it is not deadlock-free!


C. Reasoning for two-tier tracker system

* Each LLC mem access is handled by TagXact tracker which then offloads it to
    multiple TCAcc trackers.Each TCAcc tracker is mapped to a bank of cache 
    sets, becomes serialization point. This covers deadlock corner case of
    two cache accesses requesting write access on the same cycle!


*** VI. Hardware Design ***

* TCAcc each have a vector of lock registers. Optimal number is 
    2*no. of TCAcc trackers. Lock granularity is a single bit to boost 
    parallel cache accesses and minimize blocks.


*** VII. Performance Evaluation ***

Single-core tests were done on FPGA board
Multi-core tests were done using RTL simulation


A. Description of tag use cases
1. Ability to tag call/return instructions -- a rare feature in similar
    designs. It can be used to thwart direct code injection, derive data
    tags and raise precise exceptions for control-flow checking

2. Tag return addresses on the stack

3. Tag live data on the heap
    used to detect some forms of use-after-free attacks


B. Single-core evaluation
* Using a dynamic search order shows sub-optimal CPI! Optimal CPI achieved for
    bottom-up search order. Synamic search order fails to follow memory 
    access pattern changes!

* CPI and TC accesses per kilo instructions (PKI) for (almost) all benchmarks 
    (except 433.milc)


C. Multi-core evaluation
* Dynamic search order always chooses the optimal search order when it comes 
    to CPI in this case.

* CPI and TC accesses PKI reduction, just like single-core, but only in 
    the general case! If multiple HTT-inefficient benchmarks are running
    at the same time, CPI and TC accesses PKI are increased! Probably a
    good idea to dynamically disable HTT in this case.

* Using more trackers significantly impacts CPI overhead. More TagXact and 
    TCAcc trackers can be used to mitigate the performance overhead of the
    above abnormal case (mem accesses stay the same).


D. Comparing with existing TCs
* Much fewer mem accesses than CHERI-like equivalent on a single core. Main
    reason the avoidance of redundant store and empty access. HTT level 
    doesn't matter for a dynamic search order, this is against CHERI's 
    suggestion. 

* On multi-core, same amount of memory accesses, but CHERI has a much bigger 
    CPI overhead. 

* Less memory accesses than HDFI too! Main reason is HDFI's statically
    allocated cache space...
